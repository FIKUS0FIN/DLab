[general]
resource = PUT_YOUR_VALUE_HERE
os_user = PUT_YOUR_VALUE_HERE
os_family = PUT_YOUR_VALUE_HERE
cloud_provider = PUT_YOUR_VALUE_HERE

[creds]
access_key = PUT_YOUR_VALUE_HERE
secret_access_key = PUT_YOUR_VALUE_HERE
region = us-west-2
key_name = PUT_YOUR_VALUE_HERE
key_dir = /root/keys/
security_groups_ids = PUT_YOUR_VALUE_HERE
subnet_id = PUT_YOUR_VALUE_HERE
vpc_id = PUT_YOUR_VALUE_HERE
iam_user = PUT_YOUR_VALUE_HERE

[system]
local_log_dir = /response
local_api_dir = /response

[ops]
lifecycle_stage = dev

[conf]
service_base_name = PUT_YOUR_INFRA_TAG_HERE
policy_arn = 'PUT_YOUR_VALUE_HERE'

[ssn]
ami_name = ubuntu/images/hvm-ssd/ubuntu-xenial-16.04-amd64-server-20160907.1
#ami_name = RHEL-7.3_HVM_GA-20161026-x86_64-1-Hourly2-GP2
dlab_path = /opt/dlab/

[edge]
user_name = PUT_YOUR_VALUE_HERE
vpc_id = PUT_YOUR_VALUE_HERE
instance_size = t2.medium
ami_name = ubuntu/images/hvm-ssd/ubuntu-xenial-16.04-amd64-server-20160907.1
#ami_name = RHEL-7.3_HVM_GA-20161026-x86_64-1-Hourly2-GP2

[notebook]
instance_type = PUT_YOUR_VALUE_HERE
user_name = PUT_YOUR_VALUE_HERE
ssh_user = ubuntu
instance_name = PUT_YOUR_VALUE_HERE
disk_size = 30
ami_name = ubuntu/images/hvm-ssd/ubuntu-xenial-16.04-amd64-server-20160907.1
#ami_name = RHEL-7.3_HVM_GA-20161026-x86_64-1-Hourly2-GP2
spark_version = 2.0.2
hadoop_version = 2.7

[emr]
cluster_name = PUT_YOUR_VALUE_HERE
timeout = 1500
instance_count = PUT_YOUR_VALUE_HERE
master_instance_type = PUT_YOUR_VALUE_HERE
slave_instance_type = PUT_YOUR_VALUE_HERE
version = PUT_YOUR_VALUE_HERE
ec2_role = EMR_EC2_DefaultRole
service_role = EMR_DefaultRole
excluded_spark_properties = '"spark.master", "spark.eventLog.enabled", "spark.eventLog.dir", "spark.history.fs.logDirectory", "spark.sql.warehouse.dir", "spark.driver.memory", "spark.executor.extraLibraryPath", "spark.executor.extraClassPath"'
